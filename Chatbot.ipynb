{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "## Create a Chatbox\n",
    "\n",
    "### Step 1: Prepare Project\n",
    "\n",
    "   1. Load libraries\n",
    "   2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "import operator\n",
    "import pickle\n",
    "from itertools import islice\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "lines = open('/cornell_movie_dialogs_corpus/movie_lines.txt').read().split('\\n')\n",
    "convs = open('/cornell_movie_dialogs_corpus/45000_conversations.txt').read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define Problem\n",
    "\n",
    "##### What is your task? What are your goals? What do you want to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to build a “chatbox”, a conversational model, which predicts the next sentence given the previous sentence or sentences in a conversation. The goal is to be able to have a \"normal\" conversation with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Exploratory Analysis\n",
    "\n",
    "##### Understand your data: Take a “peek” of your data, answer basic questions about the dataset. Summarise your data. Explore descriptive statistics and visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n",
       " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
       " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
       " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n",
       " \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The sentences that we will be using to train our model.\n",
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The sentences' ids, which will be processed to become our input and target data.\n",
    "convs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are using 45000 conversations as otherwise it produces a Resource Exhausted Error\n",
    "len(convs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Prepare Data\n",
    "\n",
    "##### Data Cleaning/Data Wrangling/Collect more data (if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('L479748', 'And what did you say?'),\n",
       " ('L638646', 'Oh, yes. You noticed the way she does her hair.'),\n",
       " ('L525290', 'Cool!'),\n",
       " ('L638647',\n",
       "  'Something else. My wife, Madeleine, has several pieces of jewelry that belonged to Carlotta. She inherited them. Never wore them, they were too old-fashioned... until now. Now, when she is alone, she gets them out and looks at them handles them gently, curiously... puts them on and stares at herself in the mirror... and goes into that other world... is someone else again.'),\n",
       " ('L345355',\n",
       "  'For what? Breaking my heart or ruining sex for me with any other man?')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to map each line's id with its text\n",
    "id_line = {}\n",
    "for line in lines:\n",
    "    temp_line = line.split(' +++$+++ ')\n",
    "    if len(temp_line) == 5:\n",
    "        id_line[temp_line[0]] = temp_line[4]\n",
    "list(islice(id_line.iteritems(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['L194', 'L195', 'L196', 'L197'],\n",
       " ['L198', 'L199'],\n",
       " ['L200', 'L201', 'L202', 'L203'],\n",
       " ['L204', 'L205', 'L206'],\n",
       " ['L207', 'L208']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of all of the conversations' lines' ids.\n",
    "convos = [ ]\n",
    "for line in convs[:-1]:\n",
    "    temp_line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    convos.append(temp_line.split(','))\n",
    "convos[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the sentences into questions (inputs) and answers (targets)\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for conv in convos:\n",
    "    for i in range(len(conv)-1):\n",
    "        inputs.append(id_line[conv[i]])\n",
    "        targets.append(id_line[conv[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n"
     ]
    }
   ],
   "source": [
    "# Check if we have loaded the data correctly\n",
    "for i in range(0, 3):\n",
    "    print(inputs[i])\n",
    "    print(targets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121315\n",
      "121315\n"
     ]
    }
   ],
   "source": [
    "# Compare lengths of questions and answers\n",
    "print(len(inputs))\n",
    "print(len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "clean_inputs = []\n",
    "for question in inputs:\n",
    "    clean_inputs.append(clean_text(question))\n",
    "    \n",
    "clean_targets = []    \n",
    "for answer in targets:\n",
    "    clean_targets.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again\n",
      "well i thought we would start with pronunciation if that is okay with you\n",
      "well i thought we would start with pronunciation if that is okay with you\n",
      "not the hacking and gagging and spitting part  please\n",
      "not the hacking and gagging and spitting part  please\n",
      "okay then how about we try out some french cuisine  saturday  night\n"
     ]
    }
   ],
   "source": [
    "# Take a look at some of the data to ensure that it has been cleaned well.\n",
    "for i in range(0, 3):\n",
    "    print(clean_inputs[i])\n",
    "    print(clean_targets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Length\n",
       "0      22\n",
       "1      14\n",
       "2       9\n",
       "3      14\n",
       "4      13"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the length of sentences\n",
    "length = []\n",
    "for inp in clean_inputs:\n",
    "    length.append(len(inp.split()))\n",
    "for targ in clean_targets:\n",
    "    length.append(len(targ.split()))\n",
    "\n",
    "# Create a dataframe so that the values can be inspected\n",
    "length = pd.DataFrame(length, columns=['Length'])\n",
    "length[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "32.0\n"
     ]
    }
   ],
   "source": [
    "# Compute the 5th and the 95th percentile. The length there is 1 and 32 respectively.\n",
    "print(np.percentile(length, 5))\n",
    "print(np.percentile(length, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove questions and answers that are shorter than 1 words and longer than 30 words.\n",
    "min_length = 1\n",
    "max_length = 30\n",
    "\n",
    "# Filter out the questions that are too short/long\n",
    "short_inputs_tmp = []\n",
    "short_targets_tmp = []\n",
    "\n",
    "i = 0\n",
    "for inp in clean_inputs:\n",
    "    if len(inp.split()) >= min_length and len(inp.split()) <= max_length:\n",
    "        short_inputs_tmp.append(inp)\n",
    "        short_targets_tmp.append(clean_targets[i])\n",
    "    i += 1\n",
    "\n",
    "# Filter out the answers that are too short/long\n",
    "short_inputs = []\n",
    "short_targets = []\n",
    "\n",
    "i = 0\n",
    "for inp in short_inputs_tmp:\n",
    "    if len(inp.split()) >= min_length and len(inp.split()) <= max_length:\n",
    "        short_inputs.append(inp)\n",
    "        short_targets.append(short_targets_tmp[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('# of inputs:', 114408)\n",
      "('# of targets:', 114408)\n"
     ]
    }
   ],
   "source": [
    "# Compare the number of lines we will use with the total number of lines.\n",
    "print(\"# of inputs:\", len(short_inputs))\n",
    "print(\"# of targets:\", len(short_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well i thought we would start with pronunciation if that is okay with you'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_targets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Feature Engineering\n",
    "\n",
    "##### Feature selection/feture engineering (as in new features)/data transformations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1729"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary for the frequency of the vocabulary\n",
    "vocabulary = {}\n",
    "for inp in short_inputs:\n",
    "    for token in inp.split():\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = 1\n",
    "        else:\n",
    "            vocabulary[token] += 1\n",
    "            \n",
    "for targ in short_targets:\n",
    "    for token in targ.split():\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = 1\n",
    "        else:\n",
    "            vocabulary[token] += 1\n",
    "\n",
    "vocabulary[\"life\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare words from the vocabulary.\n",
    "threshold = 5\n",
    "count = 0\n",
    "for k,v in vocabulary.items():\n",
    "    if v >= threshold:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Size of total vocab:', 44087)\n",
      "('Size of vocab we will use:', 13880)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of total vocab:\", len(vocabulary))\n",
    "print(\"Size of vocab we will use:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries with a unique integer for each word.\n",
    "inputs_int = {}\n",
    "targets_int = {}\n",
    "\n",
    "number = 0\n",
    "for inp, value in vocabulary.items():\n",
    "    if value >= threshold:\n",
    "        inputs_int[inp] = number\n",
    "        number += 1\n",
    "        \n",
    "number = 0\n",
    "for targ, value in vocabulary.items():\n",
    "    if value >= threshold:\n",
    "        targets_int[targ] = number\n",
    "        number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the unique tokens to the vocabulary dictionaries.\n",
    "codes = ['<PAD>','<EOS>','<UNK>','<GO>']\n",
    "\n",
    "for code in codes:\n",
    "    inputs_int[code] = len(inputs_int)\n",
    "    \n",
    "for code in codes:\n",
    "    targets_int[code] = len(targets_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inverse dictionary for vocabulary to integer.\n",
    "inputs_vocab = {value: inp for inp, value in inputs_int.items()}\n",
    "targets_vocab = {value: targ for targ, value in targets_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13884\n",
      "13884\n",
      "13884\n",
      "13884\n"
     ]
    }
   ],
   "source": [
    "# Check the length of the dictionaries.\n",
    "print(len(inputs_int))\n",
    "print(len(inputs_vocab))\n",
    "print(len(targets_int))\n",
    "print(len(targets_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the end of sentence token to the end of every answer.\n",
    "for i in range(len(short_targets)):\n",
    "    short_targets[i] += ' <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text to integers. \n",
    "# Replace any words that are not in the respective vocabulary with <UNK> \n",
    "input_sent_int = []\n",
    "for inp in short_inputs:\n",
    "    lst_int = []\n",
    "    for token in inp.split():\n",
    "        if token not in inputs_int:\n",
    "            lst_int.append(inputs_int['<UNK>'])\n",
    "        else:\n",
    "            lst_int.append(inputs_int[token])\n",
    "    input_sent_int.append(lst_int)\n",
    "    \n",
    "target_sent_int = []\n",
    "for targ in short_targets:\n",
    "    lst_int = []\n",
    "    for token in targ.split():\n",
    "        if token not in targets_int:\n",
    "            lst_int.append(targets_int['<UNK>'])\n",
    "        else:\n",
    "            lst_int.append(targets_int[token])\n",
    "    target_sent_int.append(lst_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114408\n",
      "114408\n"
     ]
    }
   ],
   "source": [
    "# Check the lengths\n",
    "print(len(input_sent_int))\n",
    "print(len(target_sent_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13884\n",
      "13884\n",
      "[5116, 12162, 5665, 7884, 11069, 5274, 7098, 5081, 12278, 5274]\n",
      "[11591, 12278, 9919, 13881]\n",
      "[12787, 8650, 5283, 5081, 3686, 7230]\n",
      "[2972, 5990, 1127, 13219, 9678, 13881]\n",
      "[1801]\n",
      "[8507, 5081, 6083, 4405, 2702, 6426, 6055, 4225, 13882, 7257, 13882, 13170, 8614, 1694, 7822, 12207, 13881]\n"
     ]
    }
   ],
   "source": [
    "# Sort the input and target sentences to avoid extra padding.\n",
    "sorted_inputs = []\n",
    "sorted_targets = []\n",
    "\n",
    "for length in range(1, max_length+1):\n",
    "    for i in enumerate(inputs_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_inputs.append(input_sent_int[i[0]])\n",
    "            sorted_targets.append(target_sent_int[i[0]])\n",
    "\n",
    "print(len(sorted_inputs))\n",
    "print(len(sorted_targets))\n",
    "\n",
    "for i in range(3):\n",
    "    print(sorted_inputs[i])\n",
    "    print(sorted_targets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Algorithm Selection\n",
    "\n",
    "##### Select a set of algorithms to apply, select evaluation metrics, and evaluate/compare algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use an encoder and a decoder, as well as the seq2seq model as this problem can be seen as a translation problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating the placeholders for the inputs to the model\n",
    "def enc_dec_model_inputs():\n",
    "    '''Create placeholders for inputs to the model'''\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    \n",
    "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
    "    max_target_len = tf.reduce_max(target_sequence_length)  \n",
    "    \n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    return input_data, targets, target_sequence_length, max_target_len, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for processing the target data and add a '<GO>' id in front of each sentence\n",
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    # get '<GO>' id\n",
    "    go_id = target_vocab_to_int['<GO>']\n",
    "    \n",
    "    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n",
    "    \n",
    "    return after_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functio for creating the encoding layer\n",
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    \"\"\"\n",
    "    :return: tuple (RNN output, RNN state)\n",
    "    \"\"\"\n",
    "    embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n",
    "                                             vocab_size=source_vocab_size, \n",
    "                                             embed_dim=encoding_embedding_size)\n",
    "    \n",
    "    stacked_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(rnn_size), keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    outputs, state = tf.nn.dynamic_rnn(stacked_cells, \n",
    "                                       embed, \n",
    "                                       dtype=tf.float32)\n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating the decoding training layer\n",
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a training process in decoding layer \n",
    "    :return: BasicDecoderOutput containing training logits and sample_id\n",
    "    \"\"\"\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "    \n",
    "    # for only input layer\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, \n",
    "                                               target_sequence_length)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "\n",
    "    # unrolling the decoder layer\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_summary_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating the decoding inference layer\n",
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a inference process in decoding layer \n",
    "    :return: BasicDecoderOutput containing inference logits and sample_id\n",
    "    \"\"\"\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
    "                                                      tf.fill([batch_size], start_of_sequence_id), \n",
    "                                                      end_of_sequence_id)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "    \n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_target_sequence_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating the decoding layer\n",
    "def decoding_layer(dec_input, encoder_state,\n",
    "                   target_sequence_length, max_target_sequence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    target_vocab_size = len(target_vocab_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        output_layer = tf.layers.Dense(target_vocab_size)\n",
    "        train_output = decoding_layer_train(encoder_state, \n",
    "                                            cells, \n",
    "                                            dec_embed_input, \n",
    "                                            target_sequence_length, \n",
    "                                            max_target_sequence_length, \n",
    "                                            output_layer, \n",
    "                                            keep_prob)\n",
    "\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        infer_output = decoding_layer_infer(encoder_state, \n",
    "                                            cells, \n",
    "                                            dec_embeddings, \n",
    "                                            target_vocab_to_int['<GO>'], \n",
    "                                            target_vocab_to_int['<EOS>'], \n",
    "                                            max_target_sequence_length, \n",
    "                                            target_vocab_size, \n",
    "                                            output_layer,\n",
    "                                            batch_size,\n",
    "                                            keep_prob)\n",
    "\n",
    "    return (train_output, infer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating the sq2seq model\n",
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence model\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    enc_outputs, enc_states = encoding_layer(input_data, \n",
    "                                             rnn_size, \n",
    "                                             num_layers, \n",
    "                                             keep_prob, \n",
    "                                             source_vocab_size, \n",
    "                                             enc_embedding_size)\n",
    "    \n",
    "    dec_input = process_decoder_input(target_data, \n",
    "                                      target_vocab_to_int, \n",
    "                                      batch_size)\n",
    "    \n",
    "    train_output, infer_output = decoding_layer(dec_input,\n",
    "                                               enc_states, \n",
    "                                               target_sequence_length, \n",
    "                                               max_target_sentence_length,\n",
    "                                               rnn_size,\n",
    "                                              num_layers,\n",
    "                                              target_vocab_to_int,\n",
    "                                              target_vocab_size,\n",
    "                                              batch_size,\n",
    "                                              keep_prob,\n",
    "                                              dec_embedding_size)\n",
    "    \n",
    "    return train_output, infer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "display_step = 100\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "\n",
    "rnn_size = 64\n",
    "num_layers = 2\n",
    "\n",
    "encoding_embedding_size = 100\n",
    "decoding_embedding_size = 100\n",
    "\n",
    "learning_rate = 0.001\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Model Training\n",
    "\n",
    "##### Apply ensembles and improve performance by hyperparameter optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'checkpoints/dev'\n",
    "max_target_sentence_length = max([len(sentence) for sentence in sorted_inputs])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, target_sequence_length, max_target_sequence_length, lr, keep_prob = enc_dec_model_inputs()\n",
    "    \n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(sorted_inputs),\n",
    "                                                   len(sorted_targets),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   targets_int)\n",
    "    \n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n",
    "    # - Returns a mask tensor representing the first N positions of each cell.\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function - weighted softmax cross entropy\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "\n",
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  100/1787 - Train Accuracy: 0.8470, Validation Accuracy: 0.8363, Loss: 1.4672\n",
      "Epoch   0 Batch  200/1787 - Train Accuracy: 0.7840, Validation Accuracy: 0.8363, Loss: 1.8051\n",
      "Epoch   0 Batch  300/1787 - Train Accuracy: 0.8342, Validation Accuracy: 0.8363, Loss: 1.1951\n",
      "Epoch   0 Batch  400/1787 - Train Accuracy: 0.0297, Validation Accuracy: 0.8363, Loss: 0.4936\n",
      "Epoch   0 Batch  500/1787 - Train Accuracy: 0.0022, Validation Accuracy: 0.8363, Loss: 0.8870\n",
      "Epoch   0 Batch  600/1787 - Train Accuracy: 0.0069, Validation Accuracy: 0.5350, Loss: 1.0083\n",
      "Epoch   0 Batch  700/1787 - Train Accuracy: 0.0036, Validation Accuracy: 0.0342, Loss: 1.5231\n",
      "Epoch   0 Batch  800/1787 - Train Accuracy: 0.0017, Validation Accuracy: 0.0000, Loss: 1.1466\n",
      "Epoch   0 Batch  900/1787 - Train Accuracy: 0.0037, Validation Accuracy: 0.0000, Loss: 1.4830\n",
      "Epoch   0 Batch 1000/1787 - Train Accuracy: 0.0095, Validation Accuracy: 0.0000, Loss: 1.4191\n",
      "Epoch   0 Batch 1100/1787 - Train Accuracy: 0.0026, Validation Accuracy: 0.0000, Loss: 0.9405\n",
      "Epoch   0 Batch 1200/1787 - Train Accuracy: 0.0072, Validation Accuracy: 0.0000, Loss: 1.8189\n",
      "Epoch   0 Batch 1300/1787 - Train Accuracy: 0.0028, Validation Accuracy: 0.0000, Loss: 0.8813\n",
      "Epoch   0 Batch 1400/1787 - Train Accuracy: 0.0039, Validation Accuracy: 0.0000, Loss: 1.2897\n",
      "Epoch   0 Batch 1500/1787 - Train Accuracy: 0.0025, Validation Accuracy: 0.0000, Loss: 0.7927\n",
      "Epoch   0 Batch 1600/1787 - Train Accuracy: 0.0043, Validation Accuracy: 0.0000, Loss: 0.9929\n",
      "Epoch   0 Batch 1700/1787 - Train Accuracy: 0.0030, Validation Accuracy: 0.0000, Loss: 1.1363\n",
      "Epoch   1 Batch  100/1787 - Train Accuracy: 0.0053, Validation Accuracy: 0.0032, Loss: 0.8393\n",
      "Epoch   1 Batch  200/1787 - Train Accuracy: 0.0025, Validation Accuracy: 0.0026, Loss: 1.2545\n",
      "Epoch   1 Batch  300/1787 - Train Accuracy: 0.0027, Validation Accuracy: 0.0026, Loss: 0.9520\n",
      "Epoch   1 Batch  400/1787 - Train Accuracy: 0.0016, Validation Accuracy: 0.0026, Loss: 0.4074\n",
      "Epoch   1 Batch  500/1787 - Train Accuracy: 0.0023, Validation Accuracy: 0.1748, Loss: 0.7992\n",
      "Epoch   1 Batch  600/1787 - Train Accuracy: 0.0066, Validation Accuracy: 0.0026, Loss: 0.9101\n",
      "Epoch   1 Batch  700/1787 - Train Accuracy: 0.0065, Validation Accuracy: 0.0026, Loss: 1.4001\n",
      "Epoch   1 Batch  800/1787 - Train Accuracy: 0.0030, Validation Accuracy: 0.0037, Loss: 1.0594\n",
      "Epoch   1 Batch  900/1787 - Train Accuracy: 0.0048, Validation Accuracy: 0.0040, Loss: 1.3777\n",
      "Epoch   1 Batch 1000/1787 - Train Accuracy: 0.0082, Validation Accuracy: 0.0898, Loss: 1.3370\n",
      "Epoch   1 Batch 1100/1787 - Train Accuracy: 0.0042, Validation Accuracy: 0.0037, Loss: 0.8808\n",
      "Epoch   1 Batch 1200/1787 - Train Accuracy: 0.0118, Validation Accuracy: 0.0040, Loss: 1.7190\n",
      "Epoch   1 Batch 1300/1787 - Train Accuracy: 0.0033, Validation Accuracy: 0.0040, Loss: 0.8219\n",
      "Epoch   1 Batch 1400/1787 - Train Accuracy: 0.0060, Validation Accuracy: 0.0040, Loss: 1.2100\n",
      "Epoch   1 Batch 1500/1787 - Train Accuracy: 0.0048, Validation Accuracy: 0.0037, Loss: 0.7342\n",
      "Epoch   1 Batch 1600/1787 - Train Accuracy: 0.0068, Validation Accuracy: 0.0032, Loss: 0.9297\n",
      "Epoch   1 Batch 1700/1787 - Train Accuracy: 0.0062, Validation Accuracy: 0.0034, Loss: 1.0658\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))\n",
    "\n",
    "# Split data to training and validation sets\n",
    "train_source = sorted_inputs[batch_size:]\n",
    "train_target = sorted_targets[batch_size:]\n",
    "valid_source = sorted_inputs[:batch_size]\n",
    "valid_target = sorted_targets[:batch_size]\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths)= next(get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             inputs_int['<PAD>'],\n",
    "                                                                                                             targets_int['<PAD>']))                                                                                                  \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            inputs_int['<PAD>'],\n",
    "                            targets_int['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(sorted_inputs) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Test the Model\n",
    "\n",
    "##### Test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_params(params):\n",
    "    with open('params.p', 'wb') as out_file:\n",
    "        pickle.dump(params, out_file)\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    with open('params.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params(save_path)\n",
    "load_path = load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
      "Input\n",
      "  Word Ids:      [4026]\n",
      "  Question: ['hello']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [11591, 4347]\n",
      "  Answer: you do\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    results = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in vocab_to_int:\n",
    "            results.append(vocab_to_int[word])\n",
    "        else:\n",
    "            results.append(vocab_to_int['<UNK>'])\n",
    "            \n",
    "    return results\n",
    "\n",
    "translate_sentence = 'hello'\n",
    "\n",
    "translate_sentence = sentence_to_seq(translate_sentence, inputs_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  Question: {}'.format([inputs_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  Answer: {}'.format(\" \".join([targets_vocab[i] for i in translate_logits])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
      "Input\n",
      "  Word Ids:      [9919, 13555, 11591]\n",
      "  Question: ['what', 'are', 'you']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [5081, 3787, 11637, 13882, 13881]\n",
      "  Answer: i am a <UNK> <EOS>\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    results = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in vocab_to_int:\n",
    "            results.append(vocab_to_int[word])\n",
    "        else:\n",
    "            results.append(vocab_to_int['<UNK>'])\n",
    "            \n",
    "    return results\n",
    "\n",
    "translate_sentence = 'what are you'\n",
    "\n",
    "translate_sentence = sentence_to_seq(translate_sentence, inputs_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  Question: {}'.format([inputs_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  Answer: {}'.format(\" \".join([targets_vocab[i] for i in translate_logits])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
